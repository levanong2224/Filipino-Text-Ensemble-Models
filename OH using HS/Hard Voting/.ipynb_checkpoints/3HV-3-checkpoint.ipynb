{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rAnJPEXtfyAH"
   },
   "source": [
    "# SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-14T07:56:41.605626Z",
     "iopub.status.busy": "2024-08-14T07:56:41.605533Z",
     "iopub.status.idle": "2024-08-14T07:56:41.718148Z",
     "shell.execute_reply": "2024-08-14T07:56:41.717896Z",
     "shell.execute_reply.started": "2024-08-14T07:56:41.605616Z"
    }
   },
   "outputs": [],
   "source": [
    "# This automates data tabulation onto google sheets \n",
    "\n",
    "import gspread\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "from googleapiclient.discovery import build\n",
    "\n",
    "import os\n",
    "\n",
    "# new directory path\n",
    "new_directory = '/Users/levan/ATENEO MASTERAL/Thesis'\n",
    "\n",
    "# Change the current working directory\n",
    "os.chdir(new_directory)\n",
    "\n",
    "# Use creds to create a client to interact with the Google Drive API\n",
    "scope = ['https://spreadsheets.google.com/feeds','https://www.googleapis.com/auth/drive']\n",
    "creds = ServiceAccountCredentials.from_json_keyfile_name('thesis-432315-12daec8d1ff6.json', scope)\n",
    "\n",
    "service = build('sheets', 'v4', credentials=creds)\n",
    "\n",
    "client = gspread.authorize(creds)\n",
    "\n",
    "spreadsheet_id = '1ReMgKOtkETAbtnnqkJy0eo4M2QfIq9fxtV-5ysZSdJ8' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "huMwc5Rqej4Z"
   },
   "source": [
    "# Hard Voting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-14T07:56:41.718620Z",
     "iopub.status.busy": "2024-08-14T07:56:41.718544Z",
     "iopub.status.idle": "2024-08-14T07:56:41.720207Z",
     "shell.execute_reply": "2024-08-14T07:56:41.719939Z",
     "shell.execute_reply.started": "2024-08-14T07:56:41.718613Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Specify the new directory path\n",
    "new_directory = '/Users/levan/ATENEO MASTERAL/Thesis/Development'\n",
    "\n",
    "# Change the current working directory\n",
    "os.chdir(new_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-08-14T07:56:41.720588Z",
     "iopub.status.busy": "2024-08-14T07:56:41.720528Z",
     "iopub.status.idle": "2024-08-14T07:56:43.333085Z",
     "shell.execute_reply": "2024-08-14T07:56:43.332823Z",
     "shell.execute_reply.started": "2024-08-14T07:56:41.720582Z"
    },
    "executionInfo": {
     "elapsed": 8623,
     "status": "ok",
     "timestamp": 1711542962088,
     "user": {
      "displayName": "LEONARD EVAN ONG",
      "userId": "13738954157101619104"
     },
     "user_tz": -480
    },
    "id": "sem7dvIrhBcS",
    "outputId": "a4b45244-dbd0-44d7-a7e6-06c6ef54678b"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AutoConfig\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "# Adjust file paths to your local system\n",
    "file_path = 'Corpus/data_b.csv'\n",
    "\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Split the data into training and test sets (70-30 split)\n",
    "train_df, test_df = train_test_split(df, test_size=0.3, random_state=42)\n",
    "\n",
    "# Extract texts and labels\n",
    "train_texts = train_df['text'].tolist()\n",
    "train_labels = train_df['label'].tolist()\n",
    "\n",
    "test_texts = test_df['text'].tolist()\n",
    "test_labels = test_df['label'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Models and Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-14T07:56:43.333606Z",
     "iopub.status.busy": "2024-08-14T07:56:43.333526Z",
     "iopub.status.idle": "2024-08-14T07:56:45.632801Z",
     "shell.execute_reply": "2024-08-14T07:56:45.632539Z",
     "shell.execute_reply.started": "2024-08-14T07:56:43.333599Z"
    },
    "executionInfo": {
     "elapsed": 8097,
     "status": "ok",
     "timestamp": 1711543494761,
     "user": {
      "displayName": "LEONARD EVAN ONG",
      "userId": "13738954157101619104"
     },
     "user_tz": -480
    },
    "id": "WJA7YA-s0ZoY"
   },
   "outputs": [],
   "source": [
    "def load_model_and_tokenizer(model_path, tokenizer_path, base_model):\n",
    "    # Load the tokenizer from the local directory\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "    \n",
    "    # Load the configuration from the base model, then update configuration if needed\n",
    "    config = AutoConfig.from_pretrained(base_model, num_labels=2)\n",
    "\n",
    "    # Initialize the model with the configuration\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_path, config=config)\n",
    "\n",
    "    # Ensure the model is in evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "model_info = {\n",
    "\n",
    "    'BERT cased': {\n",
    "        'model_path': 'BERT models/hs_bert-base-cased-finetuned',\n",
    "        'tokenizer_path': 'BERT models/hs_bert-base-cased-finetuned',\n",
    "        'base_model': 'google-bert/bert-base-cased'\n",
    "    },\n",
    "\n",
    "    'DistilBERT uncased': {\n",
    "        'model_path': 'BERT models/hs_distilbert-base-uncased-finetuned',\n",
    "        'tokenizer_path': 'BERT models/hs_distilbert-base-uncased-finetuned',\n",
    "        'base_model': 'distilbert/distilbert-base-uncased'\n",
    "    },\n",
    "\n",
    "    'DeBERTa': {\n",
    "        'model_path': 'BERT models/hs_DeBERTa-finetuned',\n",
    "        'tokenizer_path': 'BERT models/hs_DeBERTa-finetuned',\n",
    "        'base_model': 'microsoft/deberta-v3-base'\n",
    "    }\n",
    "\n",
    "}\n",
    "\n",
    "models_and_tokenizers = {name: load_model_and_tokenizer(info['model_path'], \n",
    "                                                        info['tokenizer_path'], \n",
    "                                                        info['base_model']) \n",
    "                         for name, info in model_info.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-14T07:56:45.633389Z",
     "iopub.status.busy": "2024-08-14T07:56:45.633292Z",
     "iopub.status.idle": "2024-08-14T07:56:45.635436Z",
     "shell.execute_reply": "2024-08-14T07:56:45.635219Z",
     "shell.execute_reply.started": "2024-08-14T07:56:45.633382Z"
    }
   },
   "outputs": [],
   "source": [
    "def texts_to_dataloader(texts, tokenizer, batch_size=32):\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    encodings = tokenizer(texts, truncation=True, padding=True, max_length=512, return_tensors=\"pt\")\n",
    "\n",
    "    # Move tensors to the specified device\n",
    "    input_ids = encodings['input_ids'].to(device)\n",
    "    attention_mask = encodings['attention_mask'].to(device)\n",
    "    \n",
    "    dataset = TensorDataset(encodings['input_ids'], encodings['attention_mask'])\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Hard Voting and Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-14T07:56:45.636919Z",
     "iopub.status.busy": "2024-08-14T07:56:45.636826Z",
     "iopub.status.idle": "2024-08-14T07:56:45.640912Z",
     "shell.execute_reply": "2024-08-14T07:56:45.640719Z",
     "shell.execute_reply.started": "2024-08-14T07:56:45.636913Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-08-14T07:56:45.642208Z",
     "iopub.status.busy": "2024-08-14T07:56:45.642147Z",
     "iopub.status.idle": "2024-08-14T07:57:05.288697Z",
     "shell.execute_reply": "2024-08-14T07:57:05.288345Z",
     "shell.execute_reply.started": "2024-08-14T07:56:45.642202Z"
    },
    "executionInfo": {
     "elapsed": 597765,
     "status": "ok",
     "timestamp": 1711544099100,
     "user": {
      "displayName": "LEONARD EVAN ONG",
      "userId": "13738954157101619104"
     },
     "user_tz": -480
    },
    "id": "BJBwRQqX5cGz",
    "outputId": "e75bd4e5-ea47-4f7f-9727-5b479b46d2e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 2080.11 MiB, increment: 182.09 MiB\n"
     ]
    }
   ],
   "source": [
    "%%memit\n",
    "\n",
    "def hard_voting_predict(models_and_tokenizers, texts):\n",
    "    votes = []\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    \n",
    "    # Loop through each model and its corresponding tokenizer\n",
    "    for name, (model, tokenizer) in models_and_tokenizers.items():\n",
    "        # Explicitly move each model to the MPS device if available\n",
    "        model.to(device)\n",
    "        \n",
    "        dataloader = texts_to_dataloader(texts, tokenizer)\n",
    "        model_preds = []\n",
    "        for batch in dataloader:\n",
    "            input_ids, attention_mask = batch\n",
    "            input_ids, attention_mask = input_ids.to(device), attention_mask.to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                preds = torch.argmax(outputs.logits, dim=-1)\n",
    "                model_preds.extend(preds.cpu().numpy())\n",
    "        votes.append(model_preds)\n",
    "\n",
    "    # Transpose to get lists of votes per sample\n",
    "    votes = np.array(votes).T  \n",
    "    # Perform voting\n",
    "    final_preds = []\n",
    "    for vote in votes:\n",
    "        vote_count = np.bincount(vote)\n",
    "        max_votes = np.max(vote_count)\n",
    "        candidates = np.where(vote_count == max_votes)[0]  # find all classes with the maximum votes\n",
    "        if len(candidates) > 1:\n",
    "            final_preds.append(random.choice(candidates))  # randomly choose among the candidates\n",
    "        else:\n",
    "            final_preds.append(candidates[0])  # choose the single candidate\n",
    "    return final_preds\n",
    "\n",
    "\n",
    "# Perform inference and voting\n",
    "final_predictions = hard_voting_predict(models_and_tokenizers, test_texts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-08-14T07:57:05.289264Z",
     "iopub.status.busy": "2024-08-14T07:57:05.289181Z",
     "iopub.status.idle": "2024-08-14T07:57:05.297980Z",
     "shell.execute_reply": "2024-08-14T07:57:05.297759Z",
     "shell.execute_reply.started": "2024-08-14T07:57:05.289257Z"
    },
    "executionInfo": {
     "elapsed": 353,
     "status": "ok",
     "timestamp": 1711544865406,
     "user": {
      "displayName": "LEONARD EVAN ONG",
      "userId": "13738954157101619104"
     },
     "user_tz": -480
    },
    "id": "kfOa-DfAAvim",
    "outputId": "4f0a53a2-b147-49bc-ade9-efd21112bff2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.7456931911402789\n",
      "Recall: 0.7952755905511811\n",
      "Ensemble accuracy: 0.7817890092258323\n",
      "F1 Score: 0.7696867061812024\n",
      "Confusion Matrix:\n",
      "[[1040  310]\n",
      " [ 234  909]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# Calculate precision, recall, accuracy, and F1 score\n",
    "precision = precision_score(test_labels, final_predictions, average='binary')  # Adjust 'binary' as needed\n",
    "recall = recall_score(test_labels, final_predictions, average='binary')  # Adjust 'binary' as needed\n",
    "accuracy = accuracy_score(test_labels, final_predictions)  # Use the original test_labels list\n",
    "f1 = f1_score(test_labels, final_predictions, average='binary')  # Adjust 'binary' as needed\n",
    "\n",
    "# Generate confusion matrix\n",
    "conf_matrix = confusion_matrix(test_labels, final_predictions)\n",
    "\n",
    "# Print the metrics\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"Ensemble accuracy: {accuracy}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-14T07:57:05.298379Z",
     "iopub.status.busy": "2024-08-14T07:57:05.298314Z",
     "iopub.status.idle": "2024-08-14T07:57:06.097887Z",
     "shell.execute_reply": "2024-08-14T07:57:06.096347Z",
     "shell.execute_reply.started": "2024-08-14T07:57:05.298373Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated cells count: 4\n"
     ]
    }
   ],
   "source": [
    "# SAVE TO GOOGLE SHEET\n",
    "\n",
    "# Define the range and values to update\n",
    "range_name = '3-1!B2:E2'  \n",
    "\n",
    "values = [[\n",
    "    f\"{precision * 100:.2f}\",\n",
    "    f\"{recall * 100:.2f}\",\n",
    "    f\"{accuracy * 100:.2f}\",\n",
    "    f\"{f1 * 100:.2f}\"\n",
    "]]\n",
    "\n",
    "# Prepare the request body\n",
    "body = {\n",
    "    'values': values\n",
    "}\n",
    "\n",
    "# Call the Sheets API to update the values\n",
    "result = service.spreadsheets().values().update(\n",
    "    spreadsheetId=spreadsheet_id, \n",
    "    range=range_name,\n",
    "    valueInputOption='USER_ENTERED',\n",
    "    body=body\n",
    ").execute()\n",
    "\n",
    "print('Updated cells count:', result.get('updatedCells'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validate on Data C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-14T07:57:06.099871Z",
     "iopub.status.busy": "2024-08-14T07:57:06.099402Z",
     "iopub.status.idle": "2024-08-14T07:57:06.114092Z",
     "shell.execute_reply": "2024-08-14T07:57:06.113463Z",
     "shell.execute_reply.started": "2024-08-14T07:57:06.099834Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# Load validation data\n",
    "validation_file_path = 'Corpus/data_c.csv'\n",
    "validation_df = pd.read_csv(validation_file_path)\n",
    "\n",
    "\n",
    "# Prepare the validation texts and labels\n",
    "validation_texts = validation_df['text'].tolist()\n",
    "validation_labels = validation_df['label'].values  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Hard Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-14T07:57:06.115342Z",
     "iopub.status.busy": "2024-08-14T07:57:06.115126Z",
     "iopub.status.idle": "2024-08-14T07:57:25.520068Z",
     "shell.execute_reply": "2024-08-14T07:57:25.519747Z",
     "shell.execute_reply.started": "2024-08-14T07:57:06.115325Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 2266.31 MiB, increment: 116.14 MiB\n"
     ]
    }
   ],
   "source": [
    "%%memit\n",
    "# Perform inference and voting on the validation texts\n",
    "validation_predictions = hard_voting_predict(models_and_tokenizers, validation_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-14T07:57:25.520739Z",
     "iopub.status.busy": "2024-08-14T07:57:25.520657Z",
     "iopub.status.idle": "2024-08-14T07:57:25.527261Z",
     "shell.execute_reply": "2024-08-14T07:57:25.527035Z",
     "shell.execute_reply.started": "2024-08-14T07:57:25.520732Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.7689804772234273\n",
      "Recall: 0.8244186046511628\n",
      "Validation accuracy: 0.8029236599891716\n",
      "F1 Score: 0.7957351290684624\n",
      "Confusion Matrix:\n",
      "[[774 213]\n",
      " [151 709]]\n"
     ]
    }
   ],
   "source": [
    "# Calculate precision, recall, accuracy, and F1 score\n",
    "precision = precision_score(validation_labels, validation_predictions, average='binary')  \n",
    "recall = recall_score(validation_labels, validation_predictions, average='binary')  \n",
    "accuracy = accuracy_score(validation_labels, validation_predictions)\n",
    "f1 = f1_score(validation_labels, validation_predictions, average='binary')  \n",
    "\n",
    "# Generate confusion matrix\n",
    "conf_matrix = confusion_matrix(validation_labels, validation_predictions)\n",
    "\n",
    "# Print the metrics\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"Validation accuracy: {accuracy}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-14T07:57:25.527628Z",
     "iopub.status.busy": "2024-08-14T07:57:25.527564Z",
     "iopub.status.idle": "2024-08-14T07:57:26.190200Z",
     "shell.execute_reply": "2024-08-14T07:57:26.186642Z",
     "shell.execute_reply.started": "2024-08-14T07:57:25.527622Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated cells count: 4\n"
     ]
    }
   ],
   "source": [
    "# SAVE TO GOOGLE SHEET\n",
    "\n",
    "# Define the range and values to update\n",
    "range_name = '3-1!F2:I2'  \n",
    "\n",
    "values = [[\n",
    "    f\"{precision * 100:.2f}\",\n",
    "    f\"{recall * 100:.2f}\",\n",
    "    f\"{accuracy * 100:.2f}\",\n",
    "    f\"{f1 * 100:.2f}\"\n",
    "]]\n",
    "\n",
    "# Prepare the request body\n",
    "body = {\n",
    "    'values': values\n",
    "}\n",
    "\n",
    "# Call the Sheets API to update the values\n",
    "result = service.spreadsheets().values().update(\n",
    "    spreadsheetId=spreadsheet_id, \n",
    "    range=range_name,\n",
    "    valueInputOption='USER_ENTERED',\n",
    "    body=body\n",
    ").execute()\n",
    "\n",
    "print('Updated cells count:', result.get('updatedCells'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-14T07:57:26.196171Z",
     "iopub.status.busy": "2024-08-14T07:57:26.193402Z",
     "iopub.status.idle": "2024-08-14T07:57:26.363344Z",
     "shell.execute_reply": "2024-08-14T07:57:26.363078Z",
     "shell.execute_reply.started": "2024-08-14T07:57:26.195955Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "  #altair-viz-8de2b7cdaef445648491e0afc6853064.vega-embed {\n",
       "    width: 100%;\n",
       "    display: flex;\n",
       "  }\n",
       "\n",
       "  #altair-viz-8de2b7cdaef445648491e0afc6853064.vega-embed details,\n",
       "  #altair-viz-8de2b7cdaef445648491e0afc6853064.vega-embed details summary {\n",
       "    position: relative;\n",
       "  }\n",
       "</style>\n",
       "<div id=\"altair-viz-8de2b7cdaef445648491e0afc6853064\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-8de2b7cdaef445648491e0afc6853064\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-8de2b7cdaef445648491e0afc6853064\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm/vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm/vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm/vega-lite@5.17.0?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm/vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"5.17.0\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 300, \"continuousHeight\": 300}}, \"layer\": [{\"mark\": {\"type\": \"rect\"}, \"encoding\": {\"color\": {\"field\": \"Count\", \"type\": \"quantitative\"}, \"tooltip\": [{\"field\": \"True\", \"type\": \"ordinal\"}, {\"field\": \"Predicted\", \"type\": \"ordinal\"}, {\"field\": \"Count\", \"type\": \"quantitative\"}], \"x\": {\"field\": \"Predicted\", \"sort\": [\"Hate Speech\", \"Non Hatespeech\"], \"type\": \"ordinal\"}, \"y\": {\"field\": \"True\", \"sort\": [\"Hate Speech\", \"Non Hatespeech\"], \"type\": \"ordinal\"}}, \"title\": \"3 Blending-1 DT\"}, {\"mark\": {\"type\": \"text\", \"align\": \"center\", \"baseline\": \"middle\", \"fontSize\": 12}, \"encoding\": {\"color\": {\"condition\": {\"test\": \"(datum.Count > 387.0)\", \"value\": \"white\"}, \"value\": \"black\"}, \"text\": {\"field\": \"Count\", \"type\": \"quantitative\"}, \"tooltip\": [{\"field\": \"True\", \"type\": \"ordinal\"}, {\"field\": \"Predicted\", \"type\": \"ordinal\"}, {\"field\": \"Count\", \"type\": \"quantitative\"}], \"x\": {\"field\": \"Predicted\", \"sort\": [\"Hate Speech\", \"Non Hatespeech\"], \"type\": \"ordinal\"}, \"y\": {\"field\": \"True\", \"sort\": [\"Hate Speech\", \"Non Hatespeech\"], \"type\": \"ordinal\"}}, \"title\": \"3 Blending-1 DT\"}], \"data\": {\"name\": \"data-cc28b9cc7729da7078995998062354cf\"}, \"height\": 300, \"width\": 400, \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.17.0.json\", \"datasets\": {\"data-cc28b9cc7729da7078995998062354cf\": [{\"True\": \"Hate Speech\", \"Predicted\": \"Hate Speech\", \"Count\": 774}, {\"True\": \"Non Hatespeech\", \"Predicted\": \"Hate Speech\", \"Count\": 151}, {\"True\": \"Hate Speech\", \"Predicted\": \"Non Hatespeech\", \"Count\": 213}, {\"True\": \"Non Hatespeech\", \"Predicted\": \"Non Hatespeech\", \"Count\": 709}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.LayerChart(...)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import altair as alt\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "cm = confusion_matrix(validation_labels, validation_predictions)\n",
    "\n",
    "# Define class names\n",
    "class_names = ['Hate Speech', 'Non Hatespeech']\n",
    "\n",
    "# Convert confusion matrix to DataFrame\n",
    "cm_df = pd.DataFrame(cm, index=class_names, columns=class_names).reset_index().melt(id_vars='index')\n",
    "cm_df.columns = ['True', 'Predicted', 'Count']\n",
    "\n",
    "# Ensure the order of categories\n",
    "cm_df['True'] = pd.Categorical(cm_df['True'], categories=class_names, ordered=True)\n",
    "cm_df['Predicted'] = pd.Categorical(cm_df['Predicted'], categories=class_names, ordered=True)\n",
    "\n",
    "# Create the Altair plot\n",
    "heatmap = alt.Chart(cm_df).mark_rect().encode(\n",
    "    x=alt.X('Predicted:O', sort=class_names),\n",
    "    y=alt.Y('True:O', sort=class_names),\n",
    "    color='Count:Q',\n",
    "    tooltip=['True', 'Predicted', 'Count']\n",
    ").properties(\n",
    "    width=400,\n",
    "    height=300,\n",
    "    title='3 Blending-1 DT'\n",
    ")\n",
    "\n",
    "# Add text labels\n",
    "text = heatmap.mark_text(\n",
    "    align='center',\n",
    "    baseline='middle',\n",
    "    fontSize=12\n",
    ").encode(\n",
    "    text='Count:Q',\n",
    "    color=alt.condition(\n",
    "        alt.datum.Count > cm.max() / 2,\n",
    "        alt.value('white'),\n",
    "        alt.value('black')\n",
    "    )\n",
    ")\n",
    "\n",
    "# Combine heatmap and text\n",
    "final_chart = heatmap + text\n",
    "\n",
    "# Display the plot\n",
    "final_chart.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-14T07:57:26.363810Z",
     "iopub.status.busy": "2024-08-14T07:57:26.363742Z",
     "iopub.status.idle": "2024-08-14T07:57:26.745583Z",
     "shell.execute_reply": "2024-08-14T07:57:26.745287Z",
     "shell.execute_reply.started": "2024-08-14T07:57:26.363804Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot saved to Results/Ensemble Model Results/On Hatespeech dataset/OH Using HS_Finetuned Models/Hard Voting/3HV-1.png\n"
     ]
    }
   ],
   "source": [
    "# Specify the folder path\n",
    "folder_path = os.path.expanduser('Results/Ensemble Model Results/On Hatespeech dataset/OH Using HS_Finetuned Models/Hard Voting/')\n",
    "\n",
    "# Save the plot using vl-convert\n",
    "file_path_png = os.path.join(folder_path, '3HV-1.png')\n",
    "final_chart.save(file_path_png)\n",
    "\n",
    "print(f\"Plot saved to {file_path_png}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOelzqcxikjHNiyC4eC309U",
   "gpuType": "V100",
   "machine_shape": "hm",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0638df159ea540faae91dfa869dd2569": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ef14f76cfd8d43dfbe7ee25186cc759d",
      "placeholder": "​",
      "style": "IPY_MODEL_98b190a34515469c8018d8ea86ef64e9",
      "value": "config.json: 100%"
     }
    },
    "18ca60da34f74f01877b9cdd4f9e7b3f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2152101fab2c4bcdb6de728e2727e8ab",
      "max": 604,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_86388af23c6b444599df0eccfcd09aeb",
      "value": 604
     }
    },
    "2152101fab2c4bcdb6de728e2727e8ab": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6e37394538a842949cb0eac6ba4b715e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "86388af23c6b444599df0eccfcd09aeb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "98b190a34515469c8018d8ea86ef64e9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b2bd860e5a134a4c824f1351502cd420": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6e37394538a842949cb0eac6ba4b715e",
      "placeholder": "​",
      "style": "IPY_MODEL_f04b6c43b7f0487ca4998f083cecac63",
      "value": " 604/604 [00:00&lt;00:00, 45.8kB/s]"
     }
    },
    "cd6b3893819949d6b46c44d4f34e1d44": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e3eb10c00acc4167b879e893e7784d80": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0638df159ea540faae91dfa869dd2569",
       "IPY_MODEL_18ca60da34f74f01877b9cdd4f9e7b3f",
       "IPY_MODEL_b2bd860e5a134a4c824f1351502cd420"
      ],
      "layout": "IPY_MODEL_cd6b3893819949d6b46c44d4f34e1d44"
     }
    },
    "ef14f76cfd8d43dfbe7ee25186cc759d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f04b6c43b7f0487ca4998f083cecac63": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
